{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10016552,
          "sourceType": "datasetVersion",
          "datasetId": 6167193
        },
        {
          "sourceId": 10016726,
          "sourceType": "datasetVersion",
          "datasetId": 6167330
        },
        {
          "sourceId": 10033568,
          "sourceType": "datasetVersion",
          "datasetId": 6179917
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T15:18:23.667889Z",
          "iopub.execute_input": "2024-11-27T15:18:23.668188Z",
          "iopub.status.idle": "2024-11-27T15:18:36.518071Z",
          "shell.execute_reply.started": "2024-11-27T15:18:23.668160Z",
          "shell.execute_reply": "2024-11-27T15:18:36.516940Z"
        },
        "id": "JjztBBwyf4c9",
        "outputId": "9b7ac2f8-793c-4b34-d1b8-ca69f879838c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T15:18:36.519306Z",
          "iopub.execute_input": "2024-11-27T15:18:36.519587Z",
          "iopub.status.idle": "2024-11-27T15:18:36.532220Z",
          "shell.execute_reply.started": "2024-11-27T15:18:36.519561Z",
          "shell.execute_reply": "2024-11-27T15:18:36.531396Z"
        },
        "id": "vHQqo1dWf4dD"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import zeros, int8, log\n",
        "from pylab import random\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import codecs\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T20:41:34.981064Z",
          "iopub.execute_input": "2024-11-27T20:41:34.981666Z",
          "iopub.status.idle": "2024-11-27T20:41:35.003615Z",
          "shell.execute_reply.started": "2024-11-27T20:41:34.981605Z",
          "shell.execute_reply": "2024-11-27T20:41:35.002659Z"
        },
        "id": "m9dHcOOof4dE"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "# segmentation, stopwords filtering and document-word matrix generating\n",
        "# [return]:\n",
        "# N : number of documents\n",
        "# M : length of dictionary\n",
        "# word2id : a map mapping terms to their corresponding ids\n",
        "# id2word : a map mapping ids to terms\n",
        "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
        "def preprocessing_plsa(datasetFilePath, stopwordsFilePath):\n",
        "\n",
        "    # read the stopwords file\n",
        "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
        "    stopwords = [line.strip() for line in file]\n",
        "    file.close()\n",
        "\n",
        "    # read the documents\n",
        "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
        "    documents = [document.strip() for document in file]\n",
        "    file.close()\n",
        "\n",
        "    # number of documents\n",
        "    N = len(documents)\n",
        "\n",
        "    wordCounts = [];\n",
        "    word2id = {}\n",
        "    id2word = {}\n",
        "    currentId = 0;\n",
        "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
        "    for document in documents:\n",
        "        segList = word_tokenize(document)\n",
        "        wordCount = {}\n",
        "        for word in segList:\n",
        "            word = word.lower().strip()\n",
        "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:\n",
        "                if word not in word2id.keys():\n",
        "                    word2id[word] = currentId;\n",
        "                    id2word[currentId] = word;\n",
        "                    currentId += 1;\n",
        "                if word in wordCount:\n",
        "                    wordCount[word] += 1\n",
        "                else:\n",
        "                    wordCount[word] = 1\n",
        "        wordCounts.append(wordCount);\n",
        "    # print(wordCount[:10])\n",
        "\n",
        "    # length of dictionary\n",
        "    M = len(word2id)\n",
        "\n",
        "    # generate the document-word matrix\n",
        "    X = zeros([N, M], int8)\n",
        "    for word in word2id.keys():\n",
        "        j = word2id[word]\n",
        "        for i in range(0, N):\n",
        "            if word in wordCounts[i]:\n",
        "                X[i, j] = wordCounts[i][word];\n",
        "\n",
        "    return N, M, word2id, id2word, X\n",
        "\n",
        "def initializeParameters_plsa():\n",
        "    for i in range(0, N):\n",
        "        normalization = sum(lamda[i, :])\n",
        "        for j in range(0, K):\n",
        "            lamda[i, j] /= normalization;\n",
        "\n",
        "    for i in range(0, K):\n",
        "        normalization = sum(theta[i, :])\n",
        "        for j in range(0, M):\n",
        "            theta[i, j] /= normalization;"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T20:43:06.175175Z",
          "iopub.execute_input": "2024-11-27T20:43:06.175858Z",
          "iopub.status.idle": "2024-11-27T20:43:06.184587Z",
          "shell.execute_reply.started": "2024-11-27T20:43:06.175822Z",
          "shell.execute_reply": "2024-11-27T20:43:06.183773Z"
        },
        "id": "A4DC81kmf4dE"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.05\n",
        "def EStep_plsa():\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            denominator = 0;\n",
        "            for k in range(0, K):\n",
        "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
        "                denominator += p[i, j, k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] = 0;\n",
        "            else:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] /= denominator;\n",
        "\n",
        "def MStep_plsa():\n",
        "    # update theta\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            theta[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                theta[k, j] += X[i, j] * p[i, j, k]\n",
        "            denominator += theta[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] /= denominator + beta\n",
        "\n",
        "    # update lamda\n",
        "    for i in range(0, N):\n",
        "        for k in range(0, K):\n",
        "            lamda[i, k] = 0\n",
        "            denominator = 0\n",
        "            for j in range(0, M):\n",
        "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
        "                denominator += X[i, j];\n",
        "            if denominator == 0:\n",
        "                lamda[i, k] = 1.0 / K\n",
        "            else:\n",
        "                lamda[i, k] /= denominator + beta\n",
        "\n",
        "# calculate the log likelihood\n",
        "def LogLikelihood_plsa():\n",
        "    loglikelihood = 0\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            tmp = 0\n",
        "            for k in range(0, K):\n",
        "                tmp += theta[k, j] * lamda[i, k]\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# output the params of model and top words of topics to files\n",
        "def output_plsa():\n",
        "    # document-topic distribution\n",
        "    file = codecs.open(docTopicDist,'w','utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(lamda[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # topic-word distribution\n",
        "    file = codecs.open(topicWordDist,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(theta[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # dictionary\n",
        "    file = codecs.open(dictionary,'w','utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # top words of each topic\n",
        "    file = codecs.open(topicWords,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = theta[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()"
      ],
      "metadata": {
        "_uuid": "2e3426b1-9699-4604-a78c-3fef521cfca0",
        "_cell_guid": "fb20fe1d-abd4-4c44-9429-9dbb823da30f",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-11-27T20:41:54.263915Z",
          "iopub.execute_input": "2024-11-27T20:41:54.264248Z",
          "iopub.status.idle": "2024-11-27T20:41:54.277432Z",
          "shell.execute_reply.started": "2024-11-27T20:41:54.264217Z",
          "shell.execute_reply": "2024-11-27T20:41:54.276513Z"
        },
        "id": "zrWl4SaHf4dF"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# set the default params and read the params from cmd\n",
        "datasetFilePath = '/content/paragraphs_output.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 30\n",
        "threshold = 5.0\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_eng_plsa1.txt'\n",
        "topicWordDist = 'topicWordDistribution_eng_plsa1.txt'\n",
        "dictionary = 'dictionary_eng_plsa1.dic'\n",
        "topicWords = 'topics_eng_plsa1.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "# preprocessing\n",
        "N, M, word2id, id2word, X = preprocessing_plsa(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "# lamda[i, j] : p(zj|di)\n",
        "lamda = random([N, K])\n",
        "\n",
        "# theta[i, j] : p(wj|zi)\n",
        "theta = random([K, M])\n",
        "\n",
        "# p[i, j, k] : p(zk|di,wj)\n",
        "p = zeros([N, M, K])\n",
        "\n",
        "initializeParameters_plsa()\n",
        "\n",
        "# EM algorithm\n",
        "oldLoglikelihood = 1\n",
        "newLoglikelihood = 1\n",
        "for i in range(0, maxIteration):\n",
        "    EStep_plsa()\n",
        "    MStep_plsa()\n",
        "    newLoglikelihood = LogLikelihood_plsa()\n",
        "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "        break\n",
        "    oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_plsa()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T20:43:11.359104Z",
          "iopub.execute_input": "2024-11-27T20:43:11.359413Z",
          "iopub.status.idle": "2024-11-27T20:43:20.278483Z",
          "shell.execute_reply.started": "2024-11-27T20:43:11.359386Z",
          "shell.execute_reply": "2024-11-27T20:43:20.277469Z"
        },
        "id": "WGj6FM10f4dG",
        "outputId": "3a8faf5b-42b1-4456-c35d-b45795632e36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2024-11-27 22:20:29 ]  1  iteration   -5294.924421439035\n",
            "[ 2024-11-27 22:20:31 ]  2  iteration   -5110.571736525161\n",
            "[ 2024-11-27 22:20:33 ]  3  iteration   -4865.420227563053\n",
            "[ 2024-11-27 22:20:35 ]  4  iteration   -4619.572319167931\n",
            "[ 2024-11-27 22:20:36 ]  5  iteration   -4417.756379328368\n",
            "[ 2024-11-27 22:20:39 ]  6  iteration   -4271.481623628496\n",
            "[ 2024-11-27 22:20:41 ]  7  iteration   -4176.235229866569\n",
            "[ 2024-11-27 22:20:43 ]  8  iteration   -4113.174386300537\n",
            "[ 2024-11-27 22:20:45 ]  9  iteration   -4067.440883371375\n",
            "[ 2024-11-27 22:20:47 ]  10  iteration   -4033.924164772707\n",
            "[ 2024-11-27 22:20:49 ]  11  iteration   -4007.3260892149415\n",
            "[ 2024-11-27 22:20:52 ]  12  iteration   -3983.357414010418\n",
            "[ 2024-11-27 22:20:54 ]  13  iteration   -3964.6403796353966\n",
            "[ 2024-11-27 22:20:56 ]  14  iteration   -3954.8241815164893\n",
            "[ 2024-11-27 22:20:58 ]  15  iteration   -3949.4828721988997\n",
            "[ 2024-11-27 22:21:00 ]  16  iteration   -3944.4412733809\n",
            "[ 2024-11-27 22:21:02 ]  17  iteration   -3938.878373739947\n",
            "[ 2024-11-27 22:21:05 ]  18  iteration   -3934.09358470579\n",
            "The latent space dimension is 10\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRDq-ZxyoTHW",
        "outputId": "58784d19-835f-4bfc-9cae-729693a3cf9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# segmentation, stopwords filtering and document-word matrix generating\n",
        "# [return]:\n",
        "# N : number of documents\n",
        "# M : length of dictionary\n",
        "# word2id : a map mapping terms to their corresponding ids\n",
        "# id2word : a map mapping ids to terms\n",
        "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
        "def preprocessing_hindi(datasetFilePath, stopwordsFilePath):\n",
        "\n",
        "    # read the stopwords file\n",
        "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
        "    stopwords = [line.strip() for line in file]\n",
        "    file.close()\n",
        "\n",
        "    # read the documents\n",
        "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
        "    documents = [document.strip() for document in file]\n",
        "    file.close()\n",
        "\n",
        "    # number of documents\n",
        "    N = len(documents)\n",
        "\n",
        "    wordCounts = [];\n",
        "    word2id = {}\n",
        "    id2word = {}\n",
        "    currentId = 0;\n",
        "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
        "    for document in documents:\n",
        "        segList = word_tokenize(document)\n",
        "        wordCount = {}\n",
        "        for word in segList:\n",
        "            word = word.lower().strip()\n",
        "            if len(word) > 1 and not re.search('[0-9\\u0966-\\u096F]', word) and word not in stopwords:\n",
        "                if word not in word2id.keys():\n",
        "                    word2id[word] = currentId;\n",
        "                    id2word[currentId] = word;\n",
        "                    currentId += 1;\n",
        "                if word in wordCount:\n",
        "                    wordCount[word] += 1\n",
        "                else:\n",
        "                    wordCount[word] = 1\n",
        "        wordCounts.append(wordCount);\n",
        "\n",
        "    # length of dictionary\n",
        "    M = len(word2id)\n",
        "\n",
        "    # generate the document-word matrix\n",
        "    X = zeros([N, M], int8)\n",
        "    for word in word2id.keys():\n",
        "        j = word2id[word]\n",
        "        for i in range(0, N):\n",
        "            if word in wordCounts[i]:\n",
        "                X[i, j] = wordCounts[i][word];\n",
        "\n",
        "    return N, M, word2id, id2word, X\n",
        "\n",
        "def initializeParameters_hindi():\n",
        "    for i in range(0, N):\n",
        "        normalization = sum(lamda[i, :])\n",
        "        for j in range(0, K):\n",
        "            lamda[i, j] /= normalization;\n",
        "\n",
        "    for i in range(0, K):\n",
        "        normalization = sum(theta[i, :])\n",
        "        for j in range(0, M):\n",
        "            theta[i, j] /= normalization;\n",
        "\n",
        "def EStep_hindi():\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            denominator = 0;\n",
        "            for k in range(0, K):\n",
        "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
        "                denominator += p[i, j, k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] = 0;\n",
        "            else:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] /= denominator;\n",
        "\n",
        "def MStep_hindi():\n",
        "    # update theta\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            theta[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                theta[k, j] += X[i, j] * p[i, j, k]\n",
        "            denominator += theta[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] /= denominator\n",
        "\n",
        "    # update lamda\n",
        "    for i in range(0, N):\n",
        "        for k in range(0, K):\n",
        "            lamda[i, k] = 0\n",
        "            denominator = 0\n",
        "            for j in range(0, M):\n",
        "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
        "                denominator += X[i, j];\n",
        "            if denominator == 0:\n",
        "                lamda[i, k] = 1.0 / K\n",
        "            else:\n",
        "                lamda[i, k] /= denominator\n",
        "\n",
        "# calculate the log likelihood\n",
        "def LogLikelihood_hindi():\n",
        "    loglikelihood = 0\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            tmp = 0\n",
        "            for k in range(0, K):\n",
        "                tmp += theta[k, j] * lamda[i, k]\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# output the params of model and top words of topics to files\n",
        "def output_hindi():\n",
        "    # document-topic distribution\n",
        "    file = codecs.open(docTopicDist,'w','utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(lamda[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # topic-word distribution\n",
        "    file = codecs.open(topicWordDist,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(theta[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # dictionary\n",
        "    file = codecs.open(dictionary,'w','utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # top words of each topic\n",
        "    file = codecs.open(topicWords,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = theta[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "# set the default params and read the params from cmd\n",
        "datasetFilePath = '/content/hindi.txt'\n",
        "stopwordsFilePath = '/content/Hindi_stopwords.txt'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 20\n",
        "threshold = 1.0\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_hindi.txt'\n",
        "topicWordDist = 'topicWordDistribution_hindi.txt'\n",
        "dictionary = 'dictionary_hindi.dic'\n",
        "topicWords = 'topics_hindi.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "# preprocessing\n",
        "N, M, word2id, id2word, X = preprocessing_hindi(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "# lamda[i, j] : p(zj|di)\n",
        "lamda = random([N, K])\n",
        "\n",
        "# theta[i, j] : p(wj|zi)\n",
        "theta = random([K, M])\n",
        "\n",
        "# p[i, j, k] : p(zk|di,wj)\n",
        "p = zeros([N, M, K])\n",
        "\n",
        "initializeParameters_hindi()\n",
        "\n",
        "# EM algorithm\n",
        "oldLoglikelihood = 1\n",
        "newLoglikelihood = 1\n",
        "for i in range(0, maxIteration):\n",
        "    EStep_hindi()\n",
        "    MStep_hindi()\n",
        "    newLoglikelihood = LogLikelihood_hindi()\n",
        "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "        break\n",
        "    oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_hindi()"
      ],
      "metadata": {
        "trusted": true,
        "id": "f90JolMFf4dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac21831-7fde-4503-b275-d6199fa552c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2024-11-27 22:25:14 ]  1  iteration   -7618.86813718769\n",
            "[ 2024-11-27 22:25:15 ]  2  iteration   -7469.451531227769\n",
            "[ 2024-11-27 22:25:16 ]  3  iteration   -7260.341798862027\n",
            "[ 2024-11-27 22:25:17 ]  4  iteration   -7013.581053483326\n",
            "[ 2024-11-27 22:25:19 ]  5  iteration   -6786.210089117876\n",
            "[ 2024-11-27 22:25:20 ]  6  iteration   -6600.357207200243\n",
            "[ 2024-11-27 22:25:21 ]  7  iteration   -6442.373263623059\n",
            "[ 2024-11-27 22:25:23 ]  8  iteration   -6304.695756519337\n",
            "[ 2024-11-27 22:25:24 ]  9  iteration   -6188.455909107235\n",
            "[ 2024-11-27 22:25:27 ]  10  iteration   -6107.351404932233\n",
            "[ 2024-11-27 22:25:28 ]  11  iteration   -6058.021252177478\n",
            "[ 2024-11-27 22:25:29 ]  12  iteration   -6027.845924912417\n",
            "[ 2024-11-27 22:25:30 ]  13  iteration   -6008.982738960089\n",
            "[ 2024-11-27 22:25:32 ]  14  iteration   -5994.5306043044475\n",
            "[ 2024-11-27 22:25:33 ]  15  iteration   -5979.995892705672\n",
            "[ 2024-11-27 22:25:34 ]  16  iteration   -5969.110917694076\n",
            "[ 2024-11-27 22:25:35 ]  17  iteration   -5961.6441958315845\n",
            "[ 2024-11-27 22:25:37 ]  18  iteration   -5955.036189348416\n",
            "[ 2024-11-27 22:25:39 ]  19  iteration   -5948.716559219428\n",
            "[ 2024-11-27 22:25:40 ]  20  iteration   -5943.092569561237\n",
            "The latent space dimension is 10\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.decomposition import TruncatedSVD\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# documents = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]\n",
        "\n",
        "# # raw documents to tf-idf matrix:\n",
        "# vectorizer = TfidfVectorizer(stop_words='english',\n",
        "#                              use_idf=True,\n",
        "#                              smooth_idf=True)\n",
        "# # SVD to reduce dimensionality:\n",
        "# svd_model = TruncatedSVD(n_components=100,         // num dimensions\n",
        "#                          algorithm='randomized',\n",
        "#                          n_iter=10)\n",
        "# # pipeline of tf-idf + SVD, fit to and applied to documents:\n",
        "# svd_transformer = Pipeline([('tfidf', vectorizer),\n",
        "#                             ('svd', svd_model)])\n",
        "# svd_matrix = svd_transformer.fit_transform(documents)\n",
        "\n",
        "# # svd_matrix can later be used to compare documents, compare words, or compare queries with documents"
      ],
      "metadata": {
        "trusted": true,
        "id": "5Lxy0xnKf4dI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from numpy.linalg import svd\n",
        "# import jieba\n",
        "# import re\n",
        "# import codecs\n",
        "\n",
        "# # Preprocessing Function\n",
        "# def preprocessing(datasetFilePath, stopwordsFilePath):\n",
        "#     \"\"\"\n",
        "#     Preprocess the dataset and generate a document-word matrix.\n",
        "\n",
        "#     Args:\n",
        "#     datasetFilePath : str\n",
        "#         Path to the dataset file.\n",
        "#     stopwordsFilePath : str\n",
        "#         Path to the stopwords file.\n",
        "\n",
        "#     Returns:\n",
        "#     N : int\n",
        "#         Number of documents.\n",
        "#     M : int\n",
        "#         Number of unique terms.\n",
        "#     word2id : dict\n",
        "#         Mapping from words to their IDs.\n",
        "#     id2word : dict\n",
        "#         Mapping from IDs to words.\n",
        "#     X : numpy array\n",
        "#         Document-word matrix.\n",
        "#     \"\"\"\n",
        "#     # Load stopwords\n",
        "#     with codecs.open(stopwordsFilePath, 'r', 'utf-8') as file:\n",
        "#         stopwords = set(line.strip() for line in file)\n",
        "\n",
        "#     # Load documents\n",
        "#     with codecs.open(datasetFilePath, 'r', 'utf-8') as file:\n",
        "#         documents = [line.strip() for line in file]\n",
        "\n",
        "#     word2id = {}\n",
        "#     id2word = {}\n",
        "#     currentId = 0\n",
        "#     wordCounts = []\n",
        "\n",
        "#     for document in documents:\n",
        "#         segList = jieba.cut(document)\n",
        "#         wordCount = {}\n",
        "#         for word in segList:\n",
        "#             word = word.lower().strip()\n",
        "#             if len(word) > 1 and word not in stopwords and not re.search(r'\\d', word):\n",
        "#                 if word not in word2id:\n",
        "#                     word2id[word] = currentId\n",
        "#                     id2word[currentId] = word\n",
        "#                     currentId += 1\n",
        "#                 wordCount[word] = wordCount.get(word, 0) + 1\n",
        "#         wordCounts.append(wordCount)\n",
        "\n",
        "#     N = len(documents)\n",
        "#     M = len(word2id)\n",
        "#     X = np.zeros((N, M), dtype=int)\n",
        "\n",
        "#     for i, wordCount in enumerate(wordCounts):\n",
        "#         for word, count in wordCount.items():\n",
        "#             X[i, word2id[word]] = count\n",
        "\n",
        "#     return N, M, word2id, id2word, X\n",
        "\n",
        "# # LSA Function\n",
        "# def perform_lsa(X, k):\n",
        "#     \"\"\"\n",
        "#     Perform LSA on document-word matrix X and reduce to k dimensions.\n",
        "\n",
        "#     Args:\n",
        "#     X : numpy array\n",
        "#         Document-word matrix.\n",
        "#     k : int\n",
        "#         Number of latent topics to retain.\n",
        "\n",
        "#     Returns:\n",
        "#     U_k : numpy array\n",
        "#         Truncated document-topic matrix.\n",
        "#     S_k : numpy array\n",
        "#         Truncated singular values.\n",
        "#     Vt_k : numpy array\n",
        "#         Truncated topic-word matrix (transposed).\n",
        "#     \"\"\"\n",
        "#     U, S, Vt = svd(X, full_matrices=False)\n",
        "#     U_k = U[:, :k]\n",
        "#     S_k = np.diag(S[:k])\n",
        "#     Vt_k = Vt[:k, :]\n",
        "#     return U_k, S_k, Vt_k\n",
        "\n",
        "# # Save Results Function\n",
        "# def save_lsa_results(U_k, S_k, Vt_k, id2word, docTopicDist, topicWordDist, topicWords, topicWordsNum):\n",
        "#     \"\"\"\n",
        "#     Save LSA results to files.\n",
        "\n",
        "#     Args:\n",
        "#     U_k : numpy array\n",
        "#         Truncated document-topic matrix.\n",
        "#     S_k : numpy array\n",
        "#         Truncated singular values.\n",
        "#     Vt_k : numpy array\n",
        "#         Truncated topic-word matrix (transposed).\n",
        "#     id2word : dict\n",
        "#         Dictionary mapping word IDs to terms.\n",
        "#     docTopicDist : str\n",
        "#         Path to save document-topic distribution.\n",
        "#     topicWordDist : str\n",
        "#         Path to save topic-word distribution.\n",
        "#     topicWords : str\n",
        "#         Path to save top words for each topic.\n",
        "#     topicWordsNum : int\n",
        "#         Number of top words to save for each topic.\n",
        "#     \"\"\"\n",
        "#     with codecs.open(docTopicDist, 'w', 'utf-8') as file:\n",
        "#         for i in range(U_k.shape[0]):\n",
        "#             file.write(' '.join(map(str, U_k[i, :])) + '\\n')\n",
        "\n",
        "#     with codecs.open(topicWordDist, 'w', 'utf-8') as file:\n",
        "#         for i in range(Vt_k.shape[0]):\n",
        "#             file.write(' '.join(map(str, Vt_k[i, :])) + '\\n')\n",
        "\n",
        "#     with codecs.open(topicWords, 'w', 'utf-8') as file:\n",
        "#         for i in range(Vt_k.shape[0]):\n",
        "#             topic_words = [id2word[j] for j in np.argsort(Vt_k[i, :])[-topicWordsNum:][::-1]]\n",
        "#             file.write(' '.join(topic_words) + '\\n')\n",
        "\n",
        "# # Main Script\n",
        "# if __name__ == '__main__':\n",
        "#     datasetFilePath = '/content/dataset1.txt'  # Path to your dataset\n",
        "#     stopwordsFilePath = '/content/stopwords.dic'  # Path to your stopwords file\n",
        "#     docTopicDist = 'docTopicDistribution.txt'\n",
        "#     topicWordDist = 'topicWordDistribution.txt'\n",
        "#     topicWords = 'topics.txt'\n",
        "#     topicWordsNum = 10  # Number of top words per topic\n",
        "#     K = 10  # Number of topics\n",
        "\n",
        "#     # Preprocess dataset\n",
        "#     N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "#     # Perform LSA\n",
        "#     U_k, S_k, Vt_k = perform_lsa(X, K)\n",
        "\n",
        "#     # Save results\n",
        "#     save_lsa_results(U_k, S_k, Vt_k, id2word, docTopicDist, topicWordDist, topicWords, topicWordsNum)\n",
        "\n",
        "#     print(\"LSA completed and results saved.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aVz6Rj6ff4dJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plot\n",
        "# # Perplexity Calculation\n",
        "# def calculate_perplexity(X, U_k, S_k, Vt_k):\n",
        "#     reconstructed_X = np.dot(np.dot(U_k, S_k), Vt_k)\n",
        "#     epsilon = 1e-4  # Avoid log(0)\n",
        "#     prob_X = reconstructed_X / np.sum(reconstructed_X)\n",
        "#     log_likelihood = np.sum(X * np.log(prob_X + epsilon))\n",
        "#     perplexity = np.exp(-log_likelihood / np.sum(X))\n",
        "#     return perplexity\n",
        "\n",
        "# # Compare Perplexity vs Latent Space Dimensions\n",
        "# def compare_datasets(dataset1_path, dataset2_path, stopwords_path, latent_dims):\n",
        "#     results = {\"dataset1\": [], \"dataset2\": []}\n",
        "\n",
        "#     for k in latent_dims:\n",
        "#         for dataset, dataset_path in zip([\"dataset1\", \"dataset2\"], [dataset1_path, dataset2_path]):\n",
        "#             N, M, word2id, id2word, X = preprocessing(dataset_path, stopwords_path)\n",
        "#             U_k, S_k, Vt_k = perform_lsa(X, k)\n",
        "#             perplexity = calculate_perplexity(X, U_k, S_k, Vt_k)\n",
        "#             results[dataset].append(perplexity)\n",
        "\n",
        "#     return results\n",
        "\n",
        "# # Precision-Recall Placeholder (requires ground truth, e.g., topic labels)\n",
        "# def plot_precision_recall():\n",
        "#     # This part requires a ground truth and predicted labels for evaluation.\n",
        "#     pass\n",
        "\n",
        "# # Plot Results\n",
        "# def plot_perplexity(results, latent_dims):\n",
        "#     plt.figure()\n",
        "#     for dataset, perplexities in results.items():\n",
        "#         plt.plot(latent_dims, perplexities, label=dataset)\n",
        "#     plt.xlabel(\"Latent Dimensions\")\n",
        "#     plt.ylabel(\"Perplexity\")\n",
        "#     plt.title(\"Perplexity vs Latent Dimensions\")\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "# # Main Execution\n",
        "# if __name__ == '__main__':\n",
        "#     dataset1_path = '/content/dataset1.txt'  # Path to first dataset\n",
        "#     dataset2_path = '/content/dataset2.txt'  # Path to second dataset\n",
        "#     stopwords_path = '/content/stopwords.dic'  # Path to stopwords file\n",
        "\n",
        "#     latent_dims = range(2, 21, 2)  # Test various latent dimensions (e.g., 2, 4, ..., 20)\n",
        "\n",
        "#     # Compare datasets based on perplexity\n",
        "#     results = compare_datasets(dataset1_path, dataset2_path, stopwords_path, latent_dims)\n",
        "\n",
        "#     # Plot perplexity vs latent dimensions\n",
        "#     plot_perplexity(results, latent_dims)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z5Jjb5qjf4dJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_priors(alpha=0.2):\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            theta[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                theta[k, j] = theta[k, j] + X[i, j] * p[i, j, k] + alpha\n",
        "            denominator += theta[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] /= denominator\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jXRDyc2nf4dK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def gibbs_sampling():\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            topic_probs = np.zeros(K)\n",
        "            for k in range(K):\n",
        "                topic_probs[k] = theta[k, j] * lamda[i, k]\n",
        "            p[i, j, :] = topic_probs / (np.sum(topic_probs) + 1e-10)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "W7jPnpIdf4dK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy_regularization():\n",
        "    entropy_loss = 0\n",
        "    for k in range(0, K):\n",
        "        entropy_loss += -np.sum(theta[k, :] * np.log(theta[k, :] + 1e-10))\n",
        "    return entropy_loss\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "uX3u6FV3f4dK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def initializeParameters_gibbs():\n",
        "    for i in range(0, N):\n",
        "        normalization = sum(lamda[i, :])\n",
        "        for j in range(0, K):\n",
        "            lamda[i, j] /= normalization;\n",
        "\n",
        "    for i in range(0, K):\n",
        "        normalization = sum(theta[i, :])\n",
        "        for j in range(0, M):\n",
        "            theta[i, j] /= normalization;\n",
        "\n",
        "# beta = 0.1\n",
        "def EStep_gibbs(alpha=0.001):\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            denominator = 0;\n",
        "            for k in range(0, K):\n",
        "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
        "                denominator += p[i, j, k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] = 0;\n",
        "            else:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] /= denominator;\n",
        "\n",
        "def MStep_gibbs(alpha=0.001):\n",
        "    # sparse_priors(alpha=0.1)  # Add Dirichlet priors for sparsity\n",
        "    # gibbs_sampling()  # Perform Gibbs sampling for Bayesian inference\n",
        "\n",
        "    # Update theta and lamda as before\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            theta[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                theta[k, j] += alpha + X[i, j] * p[i, j, k]\n",
        "            denominator += theta[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] /= denominator\n",
        "\n",
        "    for i in range(0, N):\n",
        "        for k in range(0, K):\n",
        "            lamda[i, k] = 0\n",
        "            denominator = 0\n",
        "            for j in range(0, M):\n",
        "                lamda[i, k] += alpha + X[i, j] * p[i, j, k]\n",
        "                denominator += X[i, j];\n",
        "            if denominator == 0:\n",
        "                lamda[i, k] = 1.0 / K\n",
        "            else:\n",
        "                lamda[i, k] /= denominator\n",
        "\n",
        "# Calculate the log likelihood\n",
        "def LogLikelihood_gibbs():\n",
        "    loglikelihood = 0\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            tmp = 0\n",
        "            for k in range(0, K):\n",
        "                tmp += theta[k, j] * lamda[i, k]\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# output the params of model and top words of topics to files\n",
        "def output_gibbs():\n",
        "    # document-topic distribution\n",
        "    file = codecs.open(docTopicDist,'w','utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(lamda[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # topic-word distribution\n",
        "    file = codecs.open(topicWordDist,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(theta[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # dictionary\n",
        "    file = codecs.open(dictionary,'w','utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # top words of each topic\n",
        "    file = codecs.open(topicWords,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = theta[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T15:33:58.304659Z",
          "iopub.execute_input": "2024-11-27T15:33:58.305062Z",
          "iopub.status.idle": "2024-11-27T15:33:58.324854Z",
          "shell.execute_reply.started": "2024-11-27T15:33:58.305029Z",
          "shell.execute_reply": "2024-11-27T15:33:58.324027Z"
        },
        "id": "-AufOLBVf4dL"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "# set the default params and read the params from cmd\n",
        "datasetFilePath = '/content/paragraphs_output.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 50\n",
        "threshold = 0\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_eng_gibbs.txt'\n",
        "topicWordDist = 'topicWordDistribution_eng_gibbs.txt'\n",
        "dictionary = 'dictionary_eng_gibbs.dic'\n",
        "topicWords = 'topics_eng_gibbs.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "# preprocessing\n",
        "N, M, word2id, id2word, X = preprocessing_plsa(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "# lamda[i, j] : p(zj|di)\n",
        "lamda = random([N, K])\n",
        "\n",
        "# theta[i, j] : p(wj|zi)\n",
        "theta = random([K, M])\n",
        "\n",
        "# p[i, j, k] : p(zk|di,wj)\n",
        "p = zeros([N, M, K])\n",
        "\n",
        "initializeParameters_gibbs()\n",
        "\n",
        "# EM algorithm\n",
        "oldLoglikelihood = 1\n",
        "newLoglikelihood = 1\n",
        "for i in range(0, maxIteration):\n",
        "    EStep_gibbs(alpha=0.001)\n",
        "    MStep_gibbs(alpha=0.001)\n",
        "    newLoglikelihood = LogLikelihood_gibbs()\n",
        "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "        break\n",
        "    oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_gibbs()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T15:34:25.209222Z",
          "iopub.execute_input": "2024-11-27T15:34:25.210034Z",
          "iopub.status.idle": "2024-11-27T15:34:25.367221Z",
          "shell.execute_reply.started": "2024-11-27T15:34:25.210000Z",
          "shell.execute_reply": "2024-11-27T15:34:25.366229Z"
        },
        "id": "GnbOVxaMf4dL",
        "outputId": "c159cbd8-1ebd-4824-8081-0cdad3833f28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2024-11-27 22:39:10 ]  1  iteration   -5127.499914296321\n",
            "[ 2024-11-27 22:39:12 ]  2  iteration   -5090.387039993633\n",
            "[ 2024-11-27 22:39:14 ]  3  iteration   -5030.785950979097\n",
            "[ 2024-11-27 22:39:16 ]  4  iteration   -4935.238139394036\n",
            "[ 2024-11-27 22:39:18 ]  5  iteration   -4797.701143633509\n",
            "[ 2024-11-27 22:39:20 ]  6  iteration   -4639.674176575345\n",
            "[ 2024-11-27 22:39:23 ]  7  iteration   -4503.5997291529675\n",
            "[ 2024-11-27 22:39:25 ]  8  iteration   -4406.223656771497\n",
            "[ 2024-11-27 22:39:27 ]  9  iteration   -4340.238629798519\n",
            "[ 2024-11-27 22:39:28 ]  10  iteration   -4293.61495367788\n",
            "[ 2024-11-27 22:39:30 ]  11  iteration   -4256.153294576782\n",
            "[ 2024-11-27 22:39:33 ]  12  iteration   -4226.421060972108\n",
            "[ 2024-11-27 22:39:35 ]  13  iteration   -4205.591416272595\n",
            "[ 2024-11-27 22:39:37 ]  14  iteration   -4190.39825882792\n",
            "[ 2024-11-27 22:39:39 ]  15  iteration   -4178.210418249416\n",
            "[ 2024-11-27 22:39:42 ]  16  iteration   -4168.307531765497\n",
            "[ 2024-11-27 22:39:44 ]  17  iteration   -4160.49470018621\n",
            "[ 2024-11-27 22:39:47 ]  18  iteration   -4154.338517698461\n",
            "[ 2024-11-27 22:39:49 ]  19  iteration   -4148.974710900258\n",
            "[ 2024-11-27 22:39:51 ]  20  iteration   -4143.1487245679855\n",
            "[ 2024-11-27 22:39:53 ]  21  iteration   -4135.787891321051\n",
            "[ 2024-11-27 22:39:55 ]  22  iteration   -4128.605006837818\n",
            "[ 2024-11-27 22:39:57 ]  23  iteration   -4124.527132357737\n",
            "[ 2024-11-27 22:40:00 ]  24  iteration   -4122.493656306754\n",
            "[ 2024-11-27 22:40:02 ]  25  iteration   -4121.03033287174\n",
            "[ 2024-11-27 22:40:04 ]  26  iteration   -4119.880667416478\n",
            "[ 2024-11-27 22:40:06 ]  27  iteration   -4119.105805279537\n",
            "[ 2024-11-27 22:40:08 ]  28  iteration   -4118.630134036154\n",
            "[ 2024-11-27 22:40:10 ]  29  iteration   -4118.311283313453\n",
            "[ 2024-11-27 22:40:13 ]  30  iteration   -4118.04968207743\n",
            "[ 2024-11-27 22:40:15 ]  31  iteration   -4117.790446604997\n",
            "[ 2024-11-27 22:40:17 ]  32  iteration   -4117.505645173308\n",
            "[ 2024-11-27 22:40:18 ]  33  iteration   -4117.186785903061\n",
            "[ 2024-11-27 22:40:20 ]  34  iteration   -4116.84361313789\n",
            "[ 2024-11-27 22:40:22 ]  35  iteration   -4116.500330456482\n",
            "[ 2024-11-27 22:40:25 ]  36  iteration   -4116.18486413562\n",
            "[ 2024-11-27 22:40:27 ]  37  iteration   -4115.9159697340265\n",
            "[ 2024-11-27 22:40:29 ]  38  iteration   -4115.697288564142\n",
            "[ 2024-11-27 22:40:31 ]  39  iteration   -4115.52154974468\n",
            "[ 2024-11-27 22:40:33 ]  40  iteration   -4115.378007303655\n",
            "[ 2024-11-27 22:40:35 ]  41  iteration   -4115.25714183784\n",
            "[ 2024-11-27 22:40:39 ]  42  iteration   -4115.152431802628\n",
            "[ 2024-11-27 22:40:41 ]  43  iteration   -4115.060453351113\n",
            "[ 2024-11-27 22:40:43 ]  44  iteration   -4114.9800886736775\n",
            "[ 2024-11-27 22:40:45 ]  45  iteration   -4114.911360402104\n",
            "[ 2024-11-27 22:40:46 ]  46  iteration   -4114.854369026587\n",
            "[ 2024-11-27 22:40:48 ]  47  iteration   -4114.80867470549\n",
            "[ 2024-11-27 22:40:52 ]  48  iteration   -4114.7731840374445\n",
            "[ 2024-11-27 22:40:53 ]  49  iteration   -4114.746362785102\n",
            "[ 2024-11-27 22:40:55 ]  50  iteration   -4114.726538092525\n",
            "The latent space dimension is 10\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def initializeParameters_diri():\n",
        "    for i in range(0, N):\n",
        "        normalization = sum(lamda[i, :])\n",
        "        for j in range(0, K):\n",
        "            lamda[i, j] /= normalization;\n",
        "\n",
        "    for i in range(0, K):\n",
        "        normalization = sum(theta[i, :])\n",
        "        for j in range(0, M):\n",
        "            theta[i, j] /= normalization;\n",
        "\n",
        "beta = 0.05\n",
        "def EStep_diri(alpha=0.001):\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            denominator = 0;\n",
        "            for k in range(0, K):\n",
        "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
        "                denominator += p[i, j, k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] = 0;\n",
        "            else:\n",
        "                for k in range(0, K):\n",
        "                    p[i, j, k] /= denominator;\n",
        "\n",
        "def MStep_diri(alpha=0.001):\n",
        "    # sparse_priors(alpha=0.1)  # Add Dirichlet priors for sparsity\n",
        "    # gibbs_sampling()  # Perform Gibbs sampling for Bayesian inference\n",
        "\n",
        "    # Update theta and lamda as before\n",
        "    for k in range(0, K):\n",
        "        denominator = 0\n",
        "        for j in range(0, M):\n",
        "            theta[k, j] = 0\n",
        "            for i in range(0, N):\n",
        "                theta[k, j] += alpha + X[i, j] * p[i, j, k]\n",
        "            denominator += theta[k, j]\n",
        "        if denominator == 0:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] = 1.0 / M\n",
        "        else:\n",
        "            for j in range(0, M):\n",
        "                theta[k, j] /= denominator + beta\n",
        "\n",
        "    for i in range(0, N):\n",
        "        for k in range(0, K):\n",
        "            lamda[i, k] = 0\n",
        "            denominator = 0\n",
        "            for j in range(0, M):\n",
        "                lamda[i, k] += alpha + X[i, j] * p[i, j, k]\n",
        "                denominator += X[i, j];\n",
        "            if denominator == 0:\n",
        "                lamda[i, k] = 1.0 / K\n",
        "            else:\n",
        "                lamda[i, k] /= denominator + beta\n",
        "\n",
        "# Calculate the log likelihood\n",
        "def LogLikelihood_diri():\n",
        "    loglikelihood = 0\n",
        "    for i in range(0, N):\n",
        "        for j in range(0, M):\n",
        "            tmp = 0\n",
        "            for k in range(0, K):\n",
        "                tmp += theta[k, j] * lamda[i, k]\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# output the params of model and top words of topics to files\n",
        "def output_diri():\n",
        "    # document-topic distribution\n",
        "    file = codecs.open(docTopicDist,'w','utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(lamda[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # topic-word distribution\n",
        "    file = codecs.open(topicWordDist,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(theta[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # dictionary\n",
        "    file = codecs.open(dictionary,'w','utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # top words of each topic\n",
        "    file = codecs.open(topicWords,'w','utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = theta[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "5a5gMIZb0zro"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the default params and read the params from cmd\n",
        "datasetFilePath = '/content/paragraphs_output.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 50\n",
        "threshold = 0.1\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_eng_diri.txt'\n",
        "topicWordDist = 'topicWordDistribution_eng_diri.txt'\n",
        "dictionary = 'dictionary_eng_diri.dic'\n",
        "topicWords = 'topics_eng_diri.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "# preprocessing\n",
        "N, M, word2id, id2word, X = preprocessing_plsa(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "# lamda[i, j] : p(zj|di)\n",
        "lamda = random([N, K])\n",
        "\n",
        "# theta[i, j] : p(wj|zi)\n",
        "theta = random([K, M])\n",
        "\n",
        "# p[i, j, k] : p(zk|di,wj)\n",
        "p = zeros([N, M, K])\n",
        "\n",
        "initializeParameters_diri()\n",
        "\n",
        "# EM algorithm\n",
        "oldLoglikelihood = 1\n",
        "newLoglikelihood = 1\n",
        "for i in range(0, maxIteration):\n",
        "    EStep_diri(alpha=0.001)\n",
        "    MStep_diri(alpha=0.001)\n",
        "    newLoglikelihood = LogLikelihood_diri()\n",
        "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
        "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
        "        break\n",
        "    oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_diri()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T22:38:52.187335Z",
          "iopub.execute_input": "2024-11-26T22:38:52.187631Z",
          "iopub.status.idle": "2024-11-26T22:38:59.047223Z",
          "shell.execute_reply.started": "2024-11-26T22:38:52.187605Z",
          "shell.execute_reply": "2024-11-26T22:38:59.046491Z"
        },
        "id": "CwP8iHVdf4dM",
        "outputId": "3f2f1105-e864-43c7-dec9-1107a6df7752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2024-11-27 22:56:48 ]  1  iteration   -5143.008538888668\n",
            "[ 2024-11-27 22:56:50 ]  2  iteration   -5116.713824116789\n",
            "[ 2024-11-27 22:56:52 ]  3  iteration   -5075.309566106987\n",
            "[ 2024-11-27 22:56:54 ]  4  iteration   -5010.09107907706\n",
            "[ 2024-11-27 22:56:57 ]  5  iteration   -4915.103699028177\n",
            "[ 2024-11-27 22:56:59 ]  6  iteration   -4791.225539777555\n",
            "[ 2024-11-27 22:57:01 ]  7  iteration   -4650.908600143332\n",
            "[ 2024-11-27 22:57:03 ]  8  iteration   -4517.324191780743\n",
            "[ 2024-11-27 22:57:05 ]  9  iteration   -4407.398128428811\n",
            "[ 2024-11-27 22:57:07 ]  10  iteration   -4328.465358248906\n",
            "[ 2024-11-27 22:57:10 ]  11  iteration   -4277.192664701674\n",
            "[ 2024-11-27 22:57:12 ]  12  iteration   -4244.802149955324\n",
            "[ 2024-11-27 22:57:14 ]  13  iteration   -4223.019289465597\n",
            "[ 2024-11-27 22:57:16 ]  14  iteration   -4207.071896238039\n",
            "[ 2024-11-27 22:57:18 ]  15  iteration   -4194.898158981157\n",
            "[ 2024-11-27 22:57:20 ]  16  iteration   -4185.448784289185\n",
            "[ 2024-11-27 22:57:23 ]  17  iteration   -4178.002868011467\n",
            "[ 2024-11-27 22:57:25 ]  18  iteration   -4171.8394838320955\n",
            "[ 2024-11-27 22:57:27 ]  19  iteration   -4166.377012255697\n",
            "[ 2024-11-27 22:57:29 ]  20  iteration   -4161.383384372392\n",
            "[ 2024-11-27 22:57:31 ]  21  iteration   -4156.917948604099\n",
            "[ 2024-11-27 22:57:33 ]  22  iteration   -4153.081527569564\n",
            "[ 2024-11-27 22:57:36 ]  23  iteration   -4149.894420211058\n",
            "[ 2024-11-27 22:57:38 ]  24  iteration   -4147.226847827981\n",
            "[ 2024-11-27 22:57:40 ]  25  iteration   -4144.918897963636\n",
            "[ 2024-11-27 22:57:42 ]  26  iteration   -4142.891489711993\n",
            "[ 2024-11-27 22:57:44 ]  27  iteration   -4141.203072417221\n",
            "[ 2024-11-27 22:57:46 ]  28  iteration   -4139.906997930436\n",
            "[ 2024-11-27 22:57:49 ]  29  iteration   -4138.915808311575\n",
            "[ 2024-11-27 22:57:51 ]  30  iteration   -4138.1438617142885\n",
            "[ 2024-11-27 22:57:53 ]  31  iteration   -4137.5670477867025\n",
            "[ 2024-11-27 22:57:54 ]  32  iteration   -4137.158187268018\n",
            "[ 2024-11-27 22:57:56 ]  33  iteration   -4136.863935244448\n",
            "[ 2024-11-27 22:57:58 ]  34  iteration   -4136.624365609107\n",
            "[ 2024-11-27 22:58:01 ]  35  iteration   -4136.386690769524\n",
            "[ 2024-11-27 22:58:03 ]  36  iteration   -4136.113697654248\n",
            "[ 2024-11-27 22:58:05 ]  37  iteration   -4135.801236229254\n",
            "[ 2024-11-27 22:58:07 ]  38  iteration   -4135.487110788197\n",
            "[ 2024-11-27 22:58:09 ]  39  iteration   -4135.214914280234\n",
            "[ 2024-11-27 22:58:12 ]  40  iteration   -4134.990356307415\n",
            "[ 2024-11-27 22:58:16 ]  41  iteration   -4134.788571135958\n",
            "[ 2024-11-27 22:58:18 ]  42  iteration   -4134.581070069143\n",
            "[ 2024-11-27 22:58:20 ]  43  iteration   -4134.3486723538945\n",
            "[ 2024-11-27 22:58:22 ]  44  iteration   -4134.085264742135\n",
            "[ 2024-11-27 22:58:24 ]  45  iteration   -4133.796193312297\n",
            "[ 2024-11-27 22:58:25 ]  46  iteration   -4133.488120165952\n",
            "[ 2024-11-27 22:58:29 ]  47  iteration   -4133.160730571323\n",
            "[ 2024-11-27 22:58:31 ]  48  iteration   -4132.819176976572\n",
            "[ 2024-11-27 22:58:33 ]  49  iteration   -4132.494035652079\n",
            "[ 2024-11-27 22:58:35 ]  50  iteration   -4132.227828452798\n",
            "The latent space dimension is 10\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing cell dsjdnsunusentusentunseutnestuesrtj\n",
        "\n",
        "\n",
        "datasetFilePath = '/content/mammals.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 50\n",
        "threshold = 0.1\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_eng_diri_test.txt'\n",
        "topicWordDist = 'topicWordDistribution_eng_diri_test.txt'\n",
        "dictionary = 'dictionary_eng_diri_test.dic'\n",
        "topicWords = 'topics_eng_diri_test.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "def output_diri_test():\n",
        "    # Document-topic distribution\n",
        "    with codecs.open(docTopicDist, 'w', 'utf-8') as file:\n",
        "        for i in range(0, N):\n",
        "            tmp = ' '.join(str(lamda[i, j]) for j in range(0, K))\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "    # Topic-word distribution\n",
        "    with codecs.open(topicWordDist, 'w', 'utf-8') as file:\n",
        "        for i in range(0, K):\n",
        "            tmp = ' '.join(str(theta[i, j]) for j in range(0, M))\n",
        "            print(type(tmp))\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "    # Dictionary\n",
        "    with codecs.open(dictionary, 'w', 'utf-8') as file:\n",
        "        for i in range(0, M):\n",
        "            file.write(id2word[i] + '\\n')\n",
        "\n",
        "    # Top words of each topic\n",
        "    with codecs.open(topicWords, 'w', 'utf-8') as file:\n",
        "        for i in range(0, K):\n",
        "            topicword = []\n",
        "            ids = theta[i, :].argsort()  # Get sorted word indices for topic i\n",
        "            for j in ids:\n",
        "                # Ensure j is within the valid range for id2word\n",
        "                if j < M:  # Check if index is valid\n",
        "                    topicword.insert(0, id2word.get(j, \"UNKNOWN\"))  # Add \"UNKNOWN\" if id2word[j] does not exist\n",
        "\n",
        "            tmp = ' '.join(topicword[0:min(topicWordsNum, len(topicword))])\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "# N, M, word2id, id2word, X = preprocessing_plsa(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_diri_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz8y1vnv3AUo",
        "outputId": "1af68599-fa41-41c2-d99c-ca81ea30b7a9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The latent space dimension is 10\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing cell dsjdnsunusentusentunseutnestuesrtj\n",
        "\n",
        "\n",
        "datasetFilePath = '/content/mammals.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10    # number of topic\n",
        "maxIteration = 50\n",
        "threshold = 0.1\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_eng_diri_test.txt'\n",
        "topicWordDist = 'topicWordDistribution_eng_diri_test.txt'\n",
        "dictionary = 'dictionary_eng_diri_test.dic'\n",
        "topicWords = 'topics_eng_diri_test.txt'\n",
        "if(len(sys.argv) == 11):\n",
        "    datasetFilePath = sys.argv[1]\n",
        "    stopwordsFilePath = sys.argv[2]\n",
        "    K = int(sys.argv[3])\n",
        "    maxIteration = int(sys.argv[4])\n",
        "    threshold = float(sys.argv[5])\n",
        "    topicWordsNum = int(sys.argv[6])\n",
        "    docTopicDist = sys.argv[7]\n",
        "    topicWordDist = sys.argv[8]\n",
        "    dictionary = sys.argv[9]\n",
        "    topicWords = sys.argv[10]\n",
        "\n",
        "def output_diri_test():\n",
        "    # Document-topic distribution\n",
        "    with codecs.open(docTopicDist, 'w', 'utf-8') as file:\n",
        "        for i in range(0, N):\n",
        "            tmp = ' '.join(str(lamda[i, j]) for j in range(0, K))\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "    # Topic-word distribution\n",
        "    with codecs.open(topicWordDist, 'w', 'utf-8') as file:\n",
        "        for i in range(0, K):\n",
        "            tmp = ' '.join(str(theta[i, j]) for j in range(0, M))\n",
        "            print(type(tmp))\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "    # Dictionary\n",
        "    with codecs.open(dictionary, 'w', 'utf-8') as file:\n",
        "        for i in range(0, M):\n",
        "            file.write(id2word[i] + '\\n')\n",
        "\n",
        "    # Top words of each topic\n",
        "    with codecs.open(topicWords, 'w', 'utf-8') as file:\n",
        "        for i in range(0, K):\n",
        "            topicword = []\n",
        "            ids = theta[i, :].argsort()  # Get sorted word indices for topic i\n",
        "            for j in ids:\n",
        "                # Ensure j is within the valid range for id2word\n",
        "                if j < M:  # Check if index is valid\n",
        "                    topicword.insert(0, id2word.get(j, \"UNKNOWN\"))  # Add \"UNKNOWN\" if id2word[j] does not exist\n",
        "\n",
        "            tmp = ' '.join(topicword[0:min(topicWordsNum, len(topicword))])\n",
        "            file.write(tmp + '\\n')\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "# N, M, word2id, id2word, X = preprocessing_plsa(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "print(f\"The latent space dimension is {K}\")\n",
        "output_diri_test()"
      ],
      "metadata": {
        "id": "A5OjRYiKGSdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_topic_coherence(theta, word2id, X, top_n=10):\n",
        "    \"\"\"\n",
        "    Evaluate the coherence of topics based on the top words.\n",
        "\n",
        "    :param theta: Topic-word distribution matrix.\n",
        "    :param word2id: Mapping of words to ids.\n",
        "    :param X: Document-term matrix.\n",
        "    :param top_n: Number of top words to consider for each topic.\n",
        "    :return: List of coherence scores for each topic.\n",
        "    \"\"\"\n",
        "    coherence_scores = []\n",
        "\n",
        "    for k in range(len(theta)):  # Iterate over topics\n",
        "        # Get the top N words for the current topic\n",
        "        top_word_ids = np.argsort(theta[k, :])[-top_n:][::-1]\n",
        "        top_words = [id2word[word_id] for word_id in top_word_ids]\n",
        "\n",
        "        # Calculate the PMI matrix for the top words in the topic\n",
        "        pmi_matrix = calculate_pmi(top_words, word2id, X)\n",
        "\n",
        "        # Sum the upper triangle of the PMI matrix (to avoid double counting)\n",
        "        coherence = np.sum(np.triu(pmi_matrix, k=1))  # Upper triangle of matrix\n",
        "        coherence_scores.append(coherence)\n",
        "\n",
        "    return coherence_scores\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "4E6HYMlvf4dM"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "def output_plsa_with_coherence():\n",
        "    # Document-topic distribution\n",
        "    file = codecs.open(docTopicDist, 'w', 'utf-8')\n",
        "    for i in range(0, N):\n",
        "        tmp = ''\n",
        "        for j in range(0, K):\n",
        "            tmp += str(lamda[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # Topic-word distribution\n",
        "    file = codecs.open(topicWordDist, 'w', 'utf-8')\n",
        "    for i in range(0, K):\n",
        "        tmp = ''\n",
        "        for j in range(0, M):\n",
        "            tmp += str(theta[i, j]) + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # Dictionary\n",
        "    file = codecs.open(dictionary, 'w', 'utf-8')\n",
        "    for i in range(0, M):\n",
        "        file.write(id2word[i] + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # Top words of each topic\n",
        "    file = codecs.open(topicWords, 'w', 'utf-8')\n",
        "    for i in range(0, K):\n",
        "        topicword = []\n",
        "        ids = theta[i, :].argsort()\n",
        "        for j in ids:\n",
        "            topicword.insert(0, id2word[j])\n",
        "        tmp = ''\n",
        "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
        "            tmp += word + ' '\n",
        "        file.write(tmp + '\\n')\n",
        "    file.close()\n",
        "\n",
        "    # Calculate and output coherence scores\n",
        "    coherence_scores = evaluate_topic_coherence(theta, word2id, X, top_n=topicWordsNum)\n",
        "    file = codecs.open('topic_coherence.txt', 'w', 'utf-8')\n",
        "    for i in range(K):\n",
        "        file.write(f\"Topic {i+1} coherence score: {coherence_scores[i]}\\n\")\n",
        "    file.close()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HR02nEr_f4dM"
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import codecs\n",
        "import time\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy import zeros, random\n",
        "from math import log\n",
        "import sys\n",
        "\n",
        "# Preprocessing function (no change, same as your current one)\n",
        "def preprocessing_da(datasetFilePath, stopwordsFilePath):\n",
        "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
        "    stopwords = [line.strip() for line in file]\n",
        "    file.close()\n",
        "\n",
        "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
        "    documents = [document.strip() for document in file]\n",
        "    file.close()\n",
        "\n",
        "    N = len(documents)\n",
        "    wordCounts = []\n",
        "    word2id = {}\n",
        "    id2word = {}\n",
        "    currentId = 0\n",
        "    for document in documents:\n",
        "        segList = word_tokenize(document)\n",
        "        wordCount = {}\n",
        "        for word in segList:\n",
        "            word = word.lower().strip()\n",
        "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:\n",
        "                if word not in word2id:\n",
        "                    word2id[word] = currentId\n",
        "                    id2word[currentId] = word\n",
        "                    currentId += 1\n",
        "                if word in wordCount:\n",
        "                    wordCount[word] += 1\n",
        "                else:\n",
        "                    wordCount[word] = 1\n",
        "        wordCounts.append(wordCount)\n",
        "\n",
        "    M = len(word2id)\n",
        "    X = zeros([N, M], int)\n",
        "    for word in word2id:\n",
        "        j = word2id[word]\n",
        "        for i in range(N):\n",
        "            if word in wordCounts[i]:\n",
        "                X[i, j] = wordCounts[i][word]\n",
        "\n",
        "    return N, M, word2id, id2word, X\n",
        "\n",
        "\n",
        "# Initialize parameters for LDA (Dirichlet sampling)\n",
        "def initializeParameters_da(N, M, K, alpha=0.1, beta=0.01):\n",
        "    # Initialize topic distribution for documents (theta)\n",
        "    theta = random.dirichlet([alpha] * K, N)  # [N, K]\n",
        "\n",
        "    # Initialize word distribution for topics (phi)\n",
        "    phi = random.dirichlet([beta] * M, K)  # [K, M]\n",
        "\n",
        "    # p[i, j, k]: P(z_k | d_i, w_j) (used in E-step)\n",
        "    p = np.zeros([N, M, K])\n",
        "    return theta, phi, p\n",
        "\n",
        "# E-step of LDA (update the responsibilities p[i, j, k])\n",
        "def EStep_da(X, theta, phi, N, M, K):\n",
        "    p = np.zeros([N, M, K])\n",
        "    for i in range(N):\n",
        "        for j in range(M):\n",
        "            denominator = 0\n",
        "            for k in range(K):\n",
        "                p[i, j, k] = theta[i, k] * phi[k, j]\n",
        "                denominator += p[i, j, k]\n",
        "            if denominator > 0:\n",
        "                p[i, j, :] /= denominator\n",
        "            else:\n",
        "                p[i, j, :] = 1.0 / K\n",
        "    return p\n",
        "\n",
        "# M-step of LDA (update theta and phi)\n",
        "def MStep_da(X, p, N, M, K, beta=0.01):\n",
        "    # Update phi (topic-word distribution)\n",
        "    phi = np.zeros([K, M])\n",
        "    for k in range(K):\n",
        "        for j in range(M):\n",
        "            numerator = sum(X[i, j] * p[i, j, k] for i in range(N))\n",
        "            denominator = sum(X[i, j] for i in range(N))\n",
        "            if denominator > 0:\n",
        "                phi[k, j] = numerator / denominator\n",
        "            else:\n",
        "                phi[k, j] = 1.0 / M\n",
        "    # Normalize phi\n",
        "    for k in range(K):\n",
        "        phi[k, :] /= np.sum(phi[k, :]) + beta  # Adding beta for smoothing\n",
        "\n",
        "    # Update theta (document-topic distribution)\n",
        "    theta = np.zeros([N, K])\n",
        "    for i in range(N):\n",
        "        for k in range(K):\n",
        "            numerator = sum(X[i, j] * p[i, j, k] for j in range(M))\n",
        "            denominator = sum(X[i, j] for j in range(M))\n",
        "            if denominator > 0:\n",
        "                theta[i, k] = numerator / denominator\n",
        "            else:\n",
        "                theta[i, k] = 1.0 / K\n",
        "    # Normalize theta\n",
        "    for i in range(N):\n",
        "        theta[i, :] /= np.sum(theta[i, :]) + beta  # Normalize across topics\n",
        "\n",
        "    return theta, phi\n",
        "\n",
        "# Log-Likelihood of the LDA model\n",
        "def LogLikelihood_da(X, theta, phi, N, M, K):\n",
        "    loglikelihood = 0\n",
        "    for i in range(N):\n",
        "        for j in range(M):\n",
        "            tmp = 0\n",
        "            for k in range(K):\n",
        "                tmp += theta[i, k] * phi[k, j]\n",
        "            if tmp > 0:\n",
        "                loglikelihood += X[i, j] * log(tmp)\n",
        "    return loglikelihood\n",
        "\n",
        "# Output the results of LDA model\n",
        "def output_da(theta, phi, id2word, K, M, N, docTopicDist, topicWordDist, dictionary, topicWords, topicWordsNum=10):\n",
        "    # Document-topic distribution\n",
        "    with codecs.open(docTopicDist, 'w', 'utf-8') as file:\n",
        "        for i in range(N):\n",
        "            file.write(' '.join(map(str, theta[i, :])) + '\\n')\n",
        "\n",
        "    # Topic-word distribution\n",
        "    with codecs.open(topicWordDist, 'w', 'utf-8') as file:\n",
        "        for i in range(K):\n",
        "            file.write(' '.join(map(str, phi[i, :])) + '\\n')\n",
        "\n",
        "    # Dictionary\n",
        "    with codecs.open(dictionary, 'w', 'utf-8') as file:\n",
        "        for word in id2word.values():\n",
        "            file.write(word + '\\n')\n",
        "\n",
        "    # Top words of each topic\n",
        "    with codecs.open(topicWords, 'w', 'utf-8') as file:\n",
        "        for i in range(K):\n",
        "            topicwords = np.argsort(phi[i, :])[::-1]\n",
        "            topwords = [id2word[j] for j in topicwords[:topicWordsNum]]\n",
        "            file.write(' '.join(topwords) + '\\n')\n",
        "\n",
        "\n",
        "# LDA Algorithm (EM-based)\n",
        "def da_algorithm(datasetFilePath, stopwordsFilePath, K, maxIteration, threshold, docTopicDist, topicWordDist, dictionary, topicWords, topicWordsNum=10):\n",
        "    N, M, word2id, id2word, X = preprocessing_da(datasetFilePath, stopwordsFilePath)\n",
        "\n",
        "    # Initialize parameters with Dirichlet distribution\n",
        "    theta, phi, p = initializeParameters_da(N, M, K)\n",
        "\n",
        "    oldLoglikelihood = -np.inf\n",
        "    for iteration in range(maxIteration):\n",
        "        # E-step: update responsibilities\n",
        "        p = EStep_da(X, theta, phi, N, M, K)\n",
        "\n",
        "        # M-step: update theta and phi\n",
        "        theta, phi = MStep_da(X, p, N, M, K)\n",
        "\n",
        "        # Calculate Log-Likelihood\n",
        "        newLoglikelihood = LogLikelihood_da(X, theta, phi, N, M, K)\n",
        "\n",
        "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Iteration {iteration+1}, Log-Likelihood: {newLoglikelihood}\")\n",
        "\n",
        "        # Check for convergence\n",
        "        if abs(newLoglikelihood - oldLoglikelihood) < threshold:\n",
        "            break\n",
        "        oldLoglikelihood = newLoglikelihood\n",
        "\n",
        "    # Output results\n",
        "    output_da(theta, phi, id2word, K, M, N, docTopicDist, topicWordDist, dictionary, topicWords, topicWordsNum)\n",
        "\n",
        "# Set parameters\n",
        "datasetFilePath = '/content/paragraphs_output.txt'\n",
        "stopwordsFilePath = '/content/stopwords.dic'\n",
        "K = 10  # Number of topics\n",
        "maxIteration = 200\n",
        "threshold = 1.0\n",
        "topicWordsNum = 10\n",
        "docTopicDist = 'docTopicDistribution_da.txt'\n",
        "topicWordDist = 'topicWordDistribution_da.txt'\n",
        "dictionary = 'dictionary_da.dic'\n",
        "topicWords = 'topics_da.txt'\n",
        "\n",
        "# Run LDA\n",
        "da_algorithm(datasetFilePath, stopwordsFilePath, K, maxIteration, threshold, docTopicDist, topicWordDist, dictionary, topicWords)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T20:31:11.878944Z",
          "iopub.execute_input": "2024-11-27T20:31:11.879250Z",
          "iopub.status.idle": "2024-11-27T20:32:02.299785Z",
          "shell.execute_reply.started": "2024-11-27T20:31:11.879224Z",
          "shell.execute_reply": "2024-11-27T20:32:02.298858Z"
        },
        "id": "GZRk1PRPf4dN",
        "outputId": "1daa9e4e-9061-432a-a501-29e4c2d1d6d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-27 23:34:18] Iteration 1, Log-Likelihood: -5579.276268721349\n",
            "[2024-11-27 23:34:20] Iteration 2, Log-Likelihood: -5536.527340787083\n",
            "[2024-11-27 23:34:23] Iteration 3, Log-Likelihood: -5511.815440098001\n",
            "[2024-11-27 23:34:25] Iteration 4, Log-Likelihood: -5493.710616277124\n",
            "[2024-11-27 23:34:27] Iteration 5, Log-Likelihood: -5480.70310434348\n",
            "[2024-11-27 23:34:29] Iteration 6, Log-Likelihood: -5468.897374455879\n",
            "[2024-11-27 23:34:30] Iteration 7, Log-Likelihood: -5456.867933255942\n",
            "[2024-11-27 23:34:32] Iteration 8, Log-Likelihood: -5444.392617716661\n",
            "[2024-11-27 23:34:34] Iteration 9, Log-Likelihood: -5431.053067493016\n",
            "[2024-11-27 23:34:35] Iteration 10, Log-Likelihood: -5417.567572582202\n",
            "[2024-11-27 23:34:37] Iteration 11, Log-Likelihood: -5405.254086615993\n",
            "[2024-11-27 23:34:40] Iteration 12, Log-Likelihood: -5395.96200737337\n",
            "[2024-11-27 23:34:42] Iteration 13, Log-Likelihood: -5387.2774867175085\n",
            "[2024-11-27 23:34:43] Iteration 14, Log-Likelihood: -5378.417718405353\n",
            "[2024-11-27 23:34:45] Iteration 15, Log-Likelihood: -5369.169223732603\n",
            "[2024-11-27 23:34:46] Iteration 16, Log-Likelihood: -5359.537713741255\n",
            "[2024-11-27 23:34:48] Iteration 17, Log-Likelihood: -5351.251796905074\n",
            "[2024-11-27 23:34:50] Iteration 18, Log-Likelihood: -5344.269784514811\n",
            "[2024-11-27 23:34:53] Iteration 19, Log-Likelihood: -5337.629002017374\n",
            "[2024-11-27 23:34:54] Iteration 20, Log-Likelihood: -5332.135982793441\n",
            "[2024-11-27 23:34:56] Iteration 21, Log-Likelihood: -5326.770018460374\n",
            "[2024-11-27 23:34:57] Iteration 22, Log-Likelihood: -5320.580143972291\n",
            "[2024-11-27 23:34:59] Iteration 23, Log-Likelihood: -5312.526618837945\n",
            "[2024-11-27 23:35:00] Iteration 24, Log-Likelihood: -5305.077390443918\n",
            "[2024-11-27 23:35:02] Iteration 25, Log-Likelihood: -5300.108374870473\n",
            "[2024-11-27 23:35:05] Iteration 26, Log-Likelihood: -5295.577890789345\n",
            "[2024-11-27 23:35:07] Iteration 27, Log-Likelihood: -5290.789777351732\n",
            "[2024-11-27 23:35:08] Iteration 28, Log-Likelihood: -5285.591138420064\n",
            "[2024-11-27 23:35:10] Iteration 29, Log-Likelihood: -5279.67473839971\n",
            "[2024-11-27 23:35:12] Iteration 30, Log-Likelihood: -5271.204263786224\n",
            "[2024-11-27 23:35:13] Iteration 31, Log-Likelihood: -5258.510977027781\n",
            "[2024-11-27 23:35:15] Iteration 32, Log-Likelihood: -5246.070245682058\n",
            "[2024-11-27 23:35:18] Iteration 33, Log-Likelihood: -5236.065995941671\n",
            "[2024-11-27 23:35:19] Iteration 34, Log-Likelihood: -5228.557063724016\n",
            "[2024-11-27 23:35:21] Iteration 35, Log-Likelihood: -5223.3701870023715\n",
            "[2024-11-27 23:35:23] Iteration 36, Log-Likelihood: -5218.156322144833\n",
            "[2024-11-27 23:35:24] Iteration 37, Log-Likelihood: -5211.951742718037\n",
            "[2024-11-27 23:35:26] Iteration 38, Log-Likelihood: -5206.631924370619\n",
            "[2024-11-27 23:35:27] Iteration 39, Log-Likelihood: -5200.549282360701\n",
            "[2024-11-27 23:35:30] Iteration 40, Log-Likelihood: -5194.562505875613\n",
            "[2024-11-27 23:35:32] Iteration 41, Log-Likelihood: -5189.244867057062\n",
            "[2024-11-27 23:35:33] Iteration 42, Log-Likelihood: -5185.389844395196\n",
            "[2024-11-27 23:35:35] Iteration 43, Log-Likelihood: -5182.937799376839\n",
            "[2024-11-27 23:35:36] Iteration 44, Log-Likelihood: -5180.183733949398\n",
            "[2024-11-27 23:35:38] Iteration 45, Log-Likelihood: -5175.537094515499\n",
            "[2024-11-27 23:35:40] Iteration 46, Log-Likelihood: -5168.201878431085\n",
            "[2024-11-27 23:35:42] Iteration 47, Log-Likelihood: -5160.162091998271\n",
            "[2024-11-27 23:35:44] Iteration 48, Log-Likelihood: -5151.978703177118\n",
            "[2024-11-27 23:35:46] Iteration 49, Log-Likelihood: -5142.641943024206\n",
            "[2024-11-27 23:35:47] Iteration 50, Log-Likelihood: -5136.155266457901\n",
            "[2024-11-27 23:35:49] Iteration 51, Log-Likelihood: -5132.101776875705\n",
            "[2024-11-27 23:35:51] Iteration 52, Log-Likelihood: -5126.838523742004\n",
            "[2024-11-27 23:35:52] Iteration 53, Log-Likelihood: -5121.056076618198\n",
            "[2024-11-27 23:35:54] Iteration 54, Log-Likelihood: -5116.594583296823\n",
            "[2024-11-27 23:35:57] Iteration 55, Log-Likelihood: -5113.300855227518\n",
            "[2024-11-27 23:35:58] Iteration 56, Log-Likelihood: -5109.981539984252\n",
            "[2024-11-27 23:36:00] Iteration 57, Log-Likelihood: -5106.504227054106\n",
            "[2024-11-27 23:36:02] Iteration 58, Log-Likelihood: -5103.227969437309\n",
            "[2024-11-27 23:36:03] Iteration 59, Log-Likelihood: -5099.724423314738\n",
            "[2024-11-27 23:36:05] Iteration 60, Log-Likelihood: -5094.8524857518405\n",
            "[2024-11-27 23:36:06] Iteration 61, Log-Likelihood: -5087.476509080552\n",
            "[2024-11-27 23:36:09] Iteration 62, Log-Likelihood: -5081.344574970712\n",
            "[2024-11-27 23:36:11] Iteration 63, Log-Likelihood: -5075.071023222847\n",
            "[2024-11-27 23:36:12] Iteration 64, Log-Likelihood: -5069.048958181521\n",
            "[2024-11-27 23:36:14] Iteration 65, Log-Likelihood: -5064.197007803659\n",
            "[2024-11-27 23:36:15] Iteration 66, Log-Likelihood: -5059.186726388595\n",
            "[2024-11-27 23:36:17] Iteration 67, Log-Likelihood: -5053.15169000069\n",
            "[2024-11-27 23:36:19] Iteration 68, Log-Likelihood: -5044.995770056382\n",
            "[2024-11-27 23:36:21] Iteration 69, Log-Likelihood: -5037.860761864089\n",
            "[2024-11-27 23:36:23] Iteration 70, Log-Likelihood: -5030.216249787308\n",
            "[2024-11-27 23:36:25] Iteration 71, Log-Likelihood: -5021.79463521874\n",
            "[2024-11-27 23:36:26] Iteration 72, Log-Likelihood: -5013.951128110551\n",
            "[2024-11-27 23:36:28] Iteration 73, Log-Likelihood: -5004.93544671145\n",
            "[2024-11-27 23:36:30] Iteration 74, Log-Likelihood: -4995.045350660995\n",
            "[2024-11-27 23:36:31] Iteration 75, Log-Likelihood: -4985.418888300493\n",
            "[2024-11-27 23:36:33] Iteration 76, Log-Likelihood: -4976.752824889951\n",
            "[2024-11-27 23:36:36] Iteration 77, Log-Likelihood: -4967.202401345039\n",
            "[2024-11-27 23:36:37] Iteration 78, Log-Likelihood: -4958.828700998143\n",
            "[2024-11-27 23:36:39] Iteration 79, Log-Likelihood: -4949.904821490484\n",
            "[2024-11-27 23:36:40] Iteration 80, Log-Likelihood: -4941.514717812576\n",
            "[2024-11-27 23:36:42] Iteration 81, Log-Likelihood: -4933.246345902844\n",
            "[2024-11-27 23:36:44] Iteration 82, Log-Likelihood: -4925.2879156879135\n",
            "[2024-11-27 23:36:45] Iteration 83, Log-Likelihood: -4918.856058880545\n",
            "[2024-11-27 23:36:48] Iteration 84, Log-Likelihood: -4913.225251950823\n",
            "[2024-11-27 23:36:50] Iteration 85, Log-Likelihood: -4907.426539252688\n",
            "[2024-11-27 23:36:51] Iteration 86, Log-Likelihood: -4902.667019022083\n",
            "[2024-11-27 23:36:53] Iteration 87, Log-Likelihood: -4895.12345405791\n",
            "[2024-11-27 23:36:54] Iteration 88, Log-Likelihood: -4887.2714764431785\n",
            "[2024-11-27 23:36:56] Iteration 89, Log-Likelihood: -4882.6993512389245\n",
            "[2024-11-27 23:36:57] Iteration 90, Log-Likelihood: -4879.6852553057915\n",
            "[2024-11-27 23:37:00] Iteration 91, Log-Likelihood: -4877.255158703609\n",
            "[2024-11-27 23:37:02] Iteration 92, Log-Likelihood: -4874.7622841505345\n",
            "[2024-11-27 23:37:04] Iteration 93, Log-Likelihood: -4871.998469612468\n",
            "[2024-11-27 23:37:05] Iteration 94, Log-Likelihood: -4869.137048629726\n",
            "[2024-11-27 23:37:07] Iteration 95, Log-Likelihood: -4865.165843723045\n",
            "[2024-11-27 23:37:08] Iteration 96, Log-Likelihood: -4859.1807913529565\n",
            "[2024-11-27 23:37:10] Iteration 97, Log-Likelihood: -4852.885493366847\n",
            "[2024-11-27 23:37:12] Iteration 98, Log-Likelihood: -4847.096977457512\n",
            "[2024-11-27 23:37:14] Iteration 99, Log-Likelihood: -4840.447206706597\n",
            "[2024-11-27 23:37:16] Iteration 100, Log-Likelihood: -4833.566589838579\n",
            "[2024-11-27 23:37:18] Iteration 101, Log-Likelihood: -4829.550760499874\n",
            "[2024-11-27 23:37:19] Iteration 102, Log-Likelihood: -4824.903591614692\n",
            "[2024-11-27 23:37:21] Iteration 103, Log-Likelihood: -4818.632094495261\n",
            "[2024-11-27 23:37:22] Iteration 104, Log-Likelihood: -4812.325758280768\n",
            "[2024-11-27 23:37:24] Iteration 105, Log-Likelihood: -4804.823518905622\n",
            "[2024-11-27 23:37:27] Iteration 106, Log-Likelihood: -4796.915092873616\n",
            "[2024-11-27 23:37:30] Iteration 107, Log-Likelihood: -4788.598831698077\n",
            "[2024-11-27 23:37:32] Iteration 108, Log-Likelihood: -4781.599433408968\n",
            "[2024-11-27 23:37:33] Iteration 109, Log-Likelihood: -4777.8903256228605\n",
            "[2024-11-27 23:37:35] Iteration 110, Log-Likelihood: -4775.725798041235\n",
            "[2024-11-27 23:37:37] Iteration 111, Log-Likelihood: -4773.231022722043\n",
            "[2024-11-27 23:37:39] Iteration 112, Log-Likelihood: -4770.568488755733\n",
            "[2024-11-27 23:37:41] Iteration 113, Log-Likelihood: -4768.731836816199\n",
            "[2024-11-27 23:37:43] Iteration 114, Log-Likelihood: -4767.558242116069\n",
            "[2024-11-27 23:37:44] Iteration 115, Log-Likelihood: -4765.4829077129525\n",
            "[2024-11-27 23:37:46] Iteration 116, Log-Likelihood: -4762.400509226955\n",
            "[2024-11-27 23:37:47] Iteration 117, Log-Likelihood: -4760.910932935947\n",
            "[2024-11-27 23:37:49] Iteration 118, Log-Likelihood: -4759.584954640628\n",
            "[2024-11-27 23:37:51] Iteration 119, Log-Likelihood: -4758.424530815415\n",
            "[2024-11-27 23:37:54] Iteration 120, Log-Likelihood: -4756.57161529515\n",
            "[2024-11-27 23:37:56] Iteration 121, Log-Likelihood: -4752.905747076255\n",
            "[2024-11-27 23:37:58] Iteration 122, Log-Likelihood: -4748.885392169665\n",
            "[2024-11-27 23:37:59] Iteration 123, Log-Likelihood: -4744.296615936717\n",
            "[2024-11-27 23:38:01] Iteration 124, Log-Likelihood: -4740.511409264861\n",
            "[2024-11-27 23:38:03] Iteration 125, Log-Likelihood: -4739.601287578754\n"
          ]
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from math import log\n",
        "\n",
        "def load_topic_words(file_path):\n",
        "    \"\"\" Load the topic words from the given file.\n",
        "    Each line in the file contains words for a topic.\n",
        "    Args:\n",
        "        file_path (str): Path to the file containing topic words.\n",
        "    Returns:\n",
        "        List of lists of words (one list per topic).\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        topics = [line.strip().split() for line in f.readlines()]\n",
        "    return topics\n",
        "\n",
        "def compute_coherence(topics, corpus):\n",
        "    \"\"\"\n",
        "    Compute UMass coherence score for the given topics.\n",
        "\n",
        "    Args:\n",
        "        topics (list of lists): Each list contains the top words for a topic.\n",
        "        corpus (list of lists): Each list contains words of a document in the corpus.\n",
        "\n",
        "    Returns:\n",
        "        float: The average coherence score across all topics.\n",
        "    \"\"\"\n",
        "    word_pairs_count = defaultdict(int)\n",
        "    doc_count = len(corpus)\n",
        "\n",
        "    # Build word co-occurrence statistics for the corpus\n",
        "    for doc in corpus:\n",
        "        seen_words = set(doc)\n",
        "        for word1 in seen_words:\n",
        "            for word2 in seen_words:\n",
        "                if word1 != word2:\n",
        "                    word_pairs_count[(word1, word2)] += 1\n",
        "\n",
        "    # Calculate coherence score for each topic\n",
        "    coherence_scores = []\n",
        "    for topic in topics:\n",
        "        coherence = 0\n",
        "        for i in range(len(topic)):\n",
        "            for j in range(i + 1, len(topic)):\n",
        "                word1, word2 = topic[i], topic[j]\n",
        "                co_occurrence = word_pairs_count.get((word1, word2), 0)\n",
        "                coherence += np.log((co_occurrence + 1) / (doc_count + 1))\n",
        "        coherence_scores.append(coherence)\n",
        "\n",
        "    return np.mean(coherence_scores)\n",
        "\n",
        "def compute_topic_similarity(topics1, topics2):\n",
        "    \"\"\" Calculate the cosine similarity between the topic-word distributions of two models.\n",
        "    Args:\n",
        "        topics1 (list of lists): Top words for each topic in model 1.\n",
        "        topics2 (list of lists): Top words for each topic in model 2.\n",
        "    Returns:\n",
        "        float: The average cosine similarity between corresponding topics from both models.\n",
        "    \"\"\"\n",
        "    num_topics = min(len(topics1), len(topics2))  # Ensure we only compare the minimum number of topics\n",
        "    similarities = []\n",
        "\n",
        "    for i in range(num_topics):\n",
        "        topic1 = topics1[i]\n",
        "        topic2 = topics2[i]\n",
        "\n",
        "        # Convert words in topics to a word vector based on word counts\n",
        "        word_set = set(topic1).union(set(topic2))  # All words in both topics\n",
        "        word_vector1 = np.zeros(len(word_set))  # Word vector for topic1\n",
        "        word_vector2 = np.zeros(len(word_set))  # Word vector for topic2\n",
        "\n",
        "        word_index = {word: idx for idx, word in enumerate(word_set)}  # Create an index map\n",
        "\n",
        "        for word in topic1:\n",
        "            word_vector1[word_index[word]] += 1  # Fill word vector for topic1\n",
        "\n",
        "        for word in topic2:\n",
        "            word_vector2[word_index[word]] += 1  # Fill word vector for topic2\n",
        "\n",
        "        print(\"1\", word_vector1)\n",
        "        print(\"2\", word_vector2)\n",
        "        # Calculate cosine similarity between word vectors\n",
        "        similarity = cosine_similarity([word_vector1], [word_vector2])[0][0]\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    return np.mean(similarities)\n",
        "\n",
        "\n",
        "def compute_topic_diversity(topics):\n",
        "    \"\"\" Calculate the diversity of topics based on the uniqueness of top words.\n",
        "    Args:\n",
        "        topics (list of lists): Each list contains the top words for a topic.\n",
        "    Returns:\n",
        "        float: The average diversity score (1 means high diversity, 0 means low diversity).\n",
        "    \"\"\"\n",
        "    unique_words = set()  # To track unique words\n",
        "    total_words = 0  # To track total words across topics\n",
        "\n",
        "    # Loop through topics and add words to the unique set\n",
        "    for topic in topics:\n",
        "        total_words += len(topic)\n",
        "        unique_words.update(topic)  # Add all words from the topic to the unique set\n",
        "\n",
        "    if total_words == 0:\n",
        "        return 0  # Avoid division by zero if no words are present (empty topics)\n",
        "\n",
        "    # Return the ratio of unique words to total words\n",
        "    return len(unique_words) / total_words\n",
        "\n",
        "def evaluate_topic_model(model1_file, model3_file, model4_file, model5_file, model6_file, corpus1, corpus2):\n",
        "    \"\"\" Evaluate two topic models based on coherence, similarity, and diversity.\n",
        "    Args:\n",
        "        model1_file (str): File containing the top words for the topics of the first model.\n",
        "        model2_file (str): File containing the top words for the topics of the second model.\n",
        "        corpus (list of lists): The corpus of documents.\n",
        "    \"\"\"\n",
        "    # Load topics for both models\n",
        "    topics1 = load_topic_words(model1_file)\n",
        "    topics3 = load_topic_words(model3_file)\n",
        "    topics4 = load_topic_words(model4_file)\n",
        "    topics5 = load_topic_words(model5_file)\n",
        "    topics6 = load_topic_words(model6_file)\n",
        "\n",
        "\n",
        "    # 1. Coherence Score (UMass)\n",
        "    coherence1 = compute_coherence(topics1, corpus1)\n",
        "    coherence3 = compute_coherence(topics3, corpus1)\n",
        "    coherence4 = compute_coherence(topics4, corpus2)\n",
        "    coherence5 = compute_coherence(topics5, corpus1)\n",
        "    coherence6 = compute_coherence(topics6, corpus1)\n",
        "    print(f\"Coherence Score Dirichlet Distribution: {coherence1}\")\n",
        "    print(f\"Coherence Score Dirichlet Priors: {coherence3}\")\n",
        "    print(f\"Coherence Score Dirichlet Priors test: {coherence4}\")\n",
        "    print(f\"Coherence Score Gibbs Sampling: {coherence5}\")\n",
        "    print(f\"Coherence Score PLSA: {coherence6}\")\n",
        "\n",
        "    # 3. Topic Diversity\n",
        "    diversity1 = compute_topic_diversity(topics1)\n",
        "    diversity3 = compute_topic_diversity(topics3)\n",
        "    diversity4 = compute_topic_diversity(topics4)\n",
        "    diversity5 = compute_topic_diversity(topics5)\n",
        "    diversity6 = compute_topic_diversity(topics6)\n",
        "\n",
        "    print(f\"Topic Diversity Dirichlet Distribution: {diversity1}\")\n",
        "    print(f\"Topic Diversity Dirichlet Priors: {diversity3}\")\n",
        "    print(f\"Topic Diversity Dirichlet Priors test: {diversity4}\")\n",
        "    print(f\"Topic Diversity Gibbs Sampling: {diversity5}\")\n",
        "    print(f\"Topic Diversity PLSA: {diversity6}\")\n",
        "\n",
        "corpus1 = [line.strip().split() for line in open('/content/paragraphs_output.txt', 'r', encoding='utf-8')]\n",
        "corpus2 = [line.strip().split() for line in open('/content/mammals.txt', 'r', encoding='utf-8')]\n",
        "\n",
        "# Paths to topic model output files\n",
        "model1_file = \"/content/topics_da.txt\"\n",
        "model3_file = \"/content/topics_eng_diri.txt\"\n",
        "model4_file = \"/content/topics_eng_diri_test.txt\"\n",
        "model5_file = \"/content/topics_eng_gibbs.txt\"\n",
        "model6_file = \"/content/topics_eng_plsa1.txt\"\n",
        "# model3_file = \"/content/topics_hindi.txt\"\n",
        "\n",
        "evaluate_topic_model(model1_file, model3_file, model4_file, model5_file, model6_file, corpus1, corpus2)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T20:59:41.980102Z",
          "iopub.execute_input": "2024-11-27T20:59:41.980859Z",
          "iopub.status.idle": "2024-11-27T20:59:42.185251Z",
          "shell.execute_reply.started": "2024-11-27T20:59:41.980826Z",
          "shell.execute_reply": "2024-11-27T20:59:42.184398Z"
        },
        "id": "-6EOCnIzf4dN",
        "outputId": "6e5b3704-847a-440c-9810-df0c82feba34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score Dirichlet Distribution: -204.79478481695554\n",
            "Coherence Score Dirichlet Priors: -197.111048610292\n",
            "Coherence Score Dirichlet Priors test: -141.24504678015742\n",
            "Coherence Score Gibbs Sampling: -196.73570681276684\n",
            "Coherence Score PLSA: -200.65266593459944\n",
            "Topic Diversity Dirichlet Distribution: 1.0\n",
            "Topic Diversity Dirichlet Priors: 0.76\n",
            "Topic Diversity Dirichlet Priors test: 0.76\n",
            "Topic Diversity Gibbs Sampling: 0.78\n",
            "Topic Diversity PLSA: 0.78\n"
          ]
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "3gODcHFYf4dO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}